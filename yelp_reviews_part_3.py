# -*- coding: utf-8 -*-
"""Yelp_Reviews_Part_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_lUWg9bB742oCXIGfmhGi6WmKBwLycsp

# Preparation
"""

from google.colab import drive
drive.mount("/content/drive")

# suppress warnings
import warnings;
warnings.filterwarnings("ignore");

# common imports
import pandas as pd
import numpy as np
import re
import glob
import os
import json
import random
import pprint as pp
import html
import nltk

from tqdm.auto import tqdm
# register pandas
tqdm.pandas()

import matplotlib
from matplotlib import pyplot as plt

import seaborn as sns
sns.set_style("darkgrid")

pd.set_option('display.max_colwidth',None)

# change to current directory
BASE_DIR = "/content/drive/MyDrive/"
os.chdir(BASE_DIR)

path = "./df_yelp_reviews.pkl"
df = pd.read_pickle(path)

df.head()

"""## Change label distribution"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['stars'] = le.fit_transform(df['stars'])

"""# Label distribution learning

## Load DeBERTa-v3-large
"""

pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification

# load tokenizer and model from transformers

tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-v3-large")
bert_model= AutoModelForSequenceClassification.from_pretrained("microsoft/deberta-v3-large",num_labels=5)

"""## Train-Val-Test Split"""

# prepare test dataset, which accounts for 10%
from sklearn.model_selection import train_test_split

X_train_original, X_test, y_train_original, y_test = train_test_split(
              df['text'].astype(str),
              df['stars'],
              test_size=0.2,
              random_state=42,
              stratify=df['stars'])

# get training and validation datasets

X_train, X_val, y_train, y_val = train_test_split(
    X_train_original,
    y_train_original,
    test_size=0.1,
    random_state=42,
    stratify=y_train_original)

# we use gaussian distribution to generate soft labels

def gaussian_distribution(y, num_classes=5, sigma=1.0):

    labels = np.arange(num_classes)
    dist = np.exp(-(labels - y) ** 2 / (2 * sigma ** 2))
    dist = dist / dist.sum()  # normalization
    return dist

y_train_soft = np.array([gaussian_distribution(y) for y in y_train])
y_val_soft   = np.array([gaussian_distribution(y) for y in y_val])
y_test_soft  = np.array([gaussian_distribution(y) for y in y_test])

"""## Embeddings"""

# define a function to get embeddings

def get_tokens(texts, tokenizer, max_len=128):

    return tokenizer(
        texts,
        padding="max_length",       # pad to the max length
        truncation=True,            # truncate extra long text
        max_length=max_len,
        return_tensors="pt"         # return tensors
    )

train_embedding = get_tokens(X_train.astype(str).tolist(), tokenizer, max_len=128)
val_embedding = get_tokens(X_val.astype(str).tolist(), tokenizer, max_len=128)
test_embedding  = get_tokens(X_test.astype(str).tolist(), tokenizer, max_len=128)

"""## Dataset & DataLoader"""

import torch
from torch.utils.data import Dataset, DataLoader

# construct datasets, include label distribution as target

class Dataset(Dataset):
    def __init__(self, tokens, soft_labels):
        self.tokens = tokens
        self.labels = torch.tensor(soft_labels, dtype=torch.float)

    def __getitem__(self, idx):
        return {
            "input_ids": self.tokens["input_ids"][idx],
            "attention_mask": self.tokens["attention_mask"][idx],
            "labels": self.labels[idx]}

    def __len__(self):
        return len(self.labels)

train_dataset = Dataset(train_embedding, y_train_soft)
val_dataset   = Dataset(val_embedding, y_val_soft)
test_dataset  = Dataset(test_embedding, y_test_soft)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)
test_loader = DataLoader(test_dataset, batch_size=16)

"""## Optimizer & Scheduler"""

from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup

# hyperparameters
epochs = 3
learning_rate = 2e-6
epsilon = 1e-8
warmup_ratio = 0.1

# steps
t_total = len(train_loader) * epochs
warmup_steps = int(t_total * warmup_ratio)

# optimizer
optimizer = AdamW(bert_model.parameters(),lr=learning_rate,eps=epsilon)

# shceduler
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=t_total
)

print(f"Total steps: {t_total}, Warmup steps: {warmup_steps}")

"""## Training"""

# KL loss function

def kl_loss(logits, soft_labels):
    log_probs = F.log_softmax(logits, dim=-1)
    return F.kl_div(log_probs, soft_labels, reduction='batchmean')

from tqdm.notebook import tqdm
from tqdm import trange
from torch.nn.utils import clip_grad_norm_
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_model.to(device)

for epoch in trange(epochs):
    bert_model.train()
    total_loss = 0.0

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", leave=True)

    for batch in progress_bar:
        optimizer.zero_grad()

        # send each item to GPU
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = bert_model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )

        # KL loss
        logits = outputs.logits
        loss = kl_loss(logits, batch["labels"])

        # back propagation
        loss.backward()

        clip_grad_norm_(bert_model.parameters(), max_norm=1.0)

        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        progress_bar.set_postfix({"Batch Loss": f"{loss.item():.4f}"})

    avg_epoch_loss = total_loss / len(train_loader)
    print(f"\nEpoch {epoch+1} | Average Training Loss: {avg_epoch_loss:.4f}")

# save model
output_dir = "outputs/deberta-v3-large-LDL"
tokenizer.save_pretrained(output_dir)
bert_model.save_pretrained(output_dir)

"""## Validation"""

# define a validation function

def validation(model, dataloader, device):
    model.eval()

    total_kl = 0
    preds = []
    soft_labels = []
    probs = []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            logits = outputs.logits
            prob = torch.softmax(logits, dim=-1)

            # calculate kl loss
            kl = kl_loss(logits, batch["labels"])
            total_kl += kl.item()

            preds.append(torch.argmax(logits, dim=-1).cpu().numpy())
            probs.append(prob.cpu().numpy())
            soft_labels.append(batch["labels"].cpu().numpy())

    return {
        "val_kl": total_kl / len(dataloader),
        "preds": np.concatenate(preds),
        "probs": np.concatenate(probs),
        "soft_labels": np.concatenate(soft_labels)}

# load the model and validate

from transformers import DebertaV2ForSequenceClassification,DebertaV2Tokenizer
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc, precision_recall_curve, f1_score, confusion_matrix, average_precision_score
from sklearn.preprocessing import label_binarize

model_path = "outputs/deberta-v3-large-LDL"
bert_model = DebertaV2ForSequenceClassification.from_pretrained("outputs/deberta-v3-large-LDL").to(device)
tokenizer = DebertaV2Tokenizer.from_pretrained("outputs/deberta-v3-large-LDL")

# validation
val_outputs = validation(bert_model, val_loader, device)

val_kl = val_outputs["val_kl"]
y_pred = val_outputs["preds"]
y_pred_proba = val_outputs["probs"]
soft_labels = val_outputs["soft_labels"]

# get true hard labels
y_val_true = y_val.values

print("Validation KL Loss:", val_kl)
print("Accuracy:", accuracy_score(y_val_true, y_pred))
print(classification_report(y_val_true, y_pred))

print(confusion_matrix(y_val_true, y_pred))

classes = sorted(np.unique(y_val_true))
y_val_bin = label_binarize(y_val_true, classes=classes)
n_classes = len(classes)

def plot_roc_auc(y_test_bin, y_pred_proba,classes):

  for i in range(n_classes):
      class_label = classes[i]

      # ROC-AUC curve
      fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
      roc_auc = auc(fpr, tpr)

      plt.figure(figsize=(5,5))
      plt.plot(fpr, tpr, label=f"ROC-AUC = {roc_auc:.4f}")
      plt.plot([0, 1], [0, 1], 'k--')
      plt.xlabel("False Positive Rate")
      plt.ylabel("True Positive Rate")
      plt.title(f"ROC-AUC Curve - Class {class_label}")
      plt.legend()
      plt.show()

# show PR-AUC curve

def plot_pr_auc(y_test_bin, y_pred_proba,classes):

  for i in range(n_classes):
      class_label = classes[i]

      precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_pred_proba[:, i])
      pr_auc = auc(recall, precision)

      plt.figure(figsize=(5,5))
      plt.plot(recall, precision, label=f"PR-AUC = {pr_auc:.4f}")
      plt.xlabel("Recall")
      plt.ylabel("Precision")
      plt.title(f"PR-AUC Curve - Class {class_label}")
      plt.legend()
      plt.show()

plot_roc_auc(y_val_bin, y_pred_proba, classes)

plot_pr_auc(y_val_bin, y_pred_proba, classes)

print('PR-AUC Score: ', average_precision_score(y_val_bin,y_pred_proba,average='macro'))

from sklearn.metrics import roc_auc_score

print('ROC-AUC Score:', roc_auc_score(y_val_true, y_pred_proba, multi_class='ovr'))

"""## Testing"""

# define testing function

def test(model, dataloader, device, y_test_true):
    model.eval()
    preds = []
    probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Testing"):
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )

            logits = outputs.logits
            prob = torch.softmax(logits, dim=-1)

            preds.append(torch.argmax(logits, dim=-1).cpu().numpy())
            probs.append(prob.cpu().numpy())

    preds = np.concatenate(preds, axis=0)
    probs = np.concatenate(probs, axis=0)

    accuracy = accuracy_score(y_test_true, preds)
    f1 = f1_score(y_test_true, preds, average="macro")

    print(f"\nTest Accuracy: {accuracy:.4f}")
    print(f"Test Macro-F1: {f1:.4f}")
    print("\nClassification Report:\n",classification_report(y_test_true, preds))

    return accuracy, f1, preds, probs

y_test_true = y_test.values
test_accuracy, test_f1, y_test_pred, probs_test = test(bert_model, test_loader, device, y_test_true)

"""## Suitable metrics

### Negative Log Likelihood
"""

# calculate negative log likelihood

def nll(probs, y_true, eps=1e-12):
    probs = np.clip(probs, eps, 1.0)
    nll = -np.log(probs[np.arange(len(y_true)), y_true]).mean()
    return nll

nll_test = nll(probs_test, y_test_true)
print(f"Test NLL: {nll_test:.4f}")

# baseline

nll_uniform = -np.log(1 / 5)
print(nll_uniform)

"""### MAE"""

probs = np.array(probs_test)
classes = np.arange(probs.shape[1])
y_pred_score = (probs * classes[None, :]).sum(axis=1)

mae = np.abs(y_pred_score - y_test_true).mean()
print("MAE:", mae)

"""### Off-by-one Accuracy"""

y_pred_hard = probs.argmax(axis=1)

off1_acc = (np.abs(y_pred_hard - y_test_true) <= 1).mean()
print("Off-by-one Accuracy:", off1_acc)